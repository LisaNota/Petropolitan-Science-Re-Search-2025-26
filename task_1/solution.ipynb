{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задание 1 — Подсчёт количества уникальных IPv6-адресов в большом файле\n",
        "\n",
        "Входные данные: текстовый файл до **10⁹ строк**, каждая строка — валидный IPv6 (полная или сокращённая форма, произвольный регистр, ведущие нули, возможен `::`), нужно посчитать число **уникальных** IPv6-адресов, если память ограничена примерно **1 ГБ** и разрешены временные файлы и оптимизации, но **запрещены сторонние библиотеки** (только стандартная библиотека Python).\n",
        "\n",
        "Ключевая идея: использовать **внешнюю сортировку (external merge sort)** по каноническому бинарному представлению IPv6 и считать уникальные при слиянии.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Через `set()` не подходит\n",
        "\n",
        "Если бы файл был маленьким, можно было бы:\n",
        "1) нормализовать каждый IPv6;\n",
        "2) складывать в `set`;\n",
        "3) взять `len(set)`.\n",
        "\n",
        "Но при $10^9$ строк это не помещается в память. Даже если хранить адреса как 16 байт (128 бит),\n",
        "это **16 ГБ** только на байты, а в Python накладные расходы на объект `bytes` и элемент `set` намного больше.\n",
        "\n",
        "Нужно решение, работающее **потоково** и выносящее тяжёлые структуры на диск.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import socket\n",
        "import heapq\n",
        "import tempfile\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "import ipaddress\n",
        "from typing import List, Tuple, Optional, BinaryIO\n",
        "\n",
        "RECORD_SIZE = 16  # IPv6 занимает ровно 16 байт в бинарном виде (128 бит)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Канонизация IPv6: строка → 16 байт\n",
        "\n",
        "По условию сравнение адресов должно выполняться в **канонической форме**:\n",
        "`8 групп по 4 hex-цифры, lower-case, с двоеточиями`.\n",
        "\n",
        "На практике даже удобнее хранить не строку, а **пакетный (packed) вид** IPv6 — ровно **16 байт**.\n",
        "Он однозначен, не зависит от регистра и сокращений (`::`), и сравнивается быстрее.\n",
        "\n",
        "Стандартная библиотека даёт функцию `socket.inet_pton(AF_INET6, s)`, которая парсит валидный IPv6\n",
        "(включая сокращённую форму) и возвращает 16 байт.\n",
        "\n",
        "Дополнительно реализовывается функция, которая превращает 16 байт обратно в каноническую строку\n",
        "в формате из условия — это удобно для проверки и отладки.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2001:0db0:0000:123a:0000:0000:0000:0030'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ipv6_to_packed(addr: str) -> bytes:\n",
        "    \"\"\"IPv6-строка (любой валидный формат) -> 16 байт (packed).\"\"\"\n",
        "    return socket.inet_pton(socket.AF_INET6, addr)\n",
        "\n",
        "def packed_to_canonical_str(packed: bytes) -> str:\n",
        "    \"\"\"16 байт -> каноническая строка: 8 групп по 4 hex, lower-case.\"\"\"\n",
        "    h = packed.hex()  # 32 hex-символа в нижнем регистре\n",
        "    return ':'.join(h[i:i+4] for i in range(0, 32, 4))\n",
        "\n",
        "# Тестовая проверка на одном адресе\n",
        "test_addr = \"2001:db0:0:123a::30\"\n",
        "p = ipv6_to_packed(test_addr)\n",
        "packed_to_canonical_str(p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Проверка на примере из условия\n",
        "\n",
        "Проверка, что две записи одного адреса дают одинаковый `packed` и одинаковую каноническую строку.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('2001:0DB0:0000:123A:0000:0000:0000:0030',\n",
              "  '2001:0db0:0000:123a:0000:0000:0000:0030',\n",
              "  True),\n",
              " ('2001:db0:0:123a::30', '2001:0db0:0000:123a:0000:0000:0000:0030', True),\n",
              " ('CD10:9A90:F9BB:E5B6:F714:86E7:F1BB:BDFC',\n",
              "  'cd10:9a90:f9bb:e5b6:f714:86e7:f1bb:bdfc',\n",
              "  False),\n",
              " ('DF96:A23D:8BA9:BAA0:A807:FB50:F9CD:B266',\n",
              "  'df96:a23d:8ba9:baa0:a807:fb50:f9cd:b266',\n",
              "  False),\n",
              " ('9D64:9DB4:B0FE:B3C2:F09F:8DE1:EC59:987D',\n",
              "  '9d64:9db4:b0fe:b3c2:f09f:8de1:ec59:987d',\n",
              "  False)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_lines = [\n",
        "    \"2001:0DB0:0000:123A:0000:0000:0000:0030\",\n",
        "    \"2001:db0:0:123a::30\",\n",
        "    \"CD10:9A90:F9BB:E5B6:F714:86E7:F1BB:BDFC\",\n",
        "    \"DF96:A23D:8BA9:BAA0:A807:FB50:F9CD:B266\",\n",
        "    \"9D64:9DB4:B0FE:B3C2:F09F:8DE1:EC59:987D\",\n",
        "]\n",
        "\n",
        "packed = [ipv6_to_packed(s) for s in example_lines]\n",
        "canon = [packed_to_canonical_str(x) for x in packed]\n",
        "\n",
        "list(zip(example_lines, canon, [x == packed[0] for x in packed]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Видно, что первые две строки действительно приводятся к одному и тому же бинарному (и каноническому) адресу.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Наивное решение (только для маленьких файлов)\n",
        "\n",
        "Наивный подсчёт уникальных для небольшого набора данных:\n",
        "- читаем строки;\n",
        "- преобразуем в 16 байт;\n",
        "- кладём в `set`.\n",
        "\n",
        "Это пригодится как **контрольная проверка** на тестах.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def count_unique_naive(input_path: str) -> int:\n",
        "    s = set()\n",
        "    with open(input_path, \"r\", encoding=\"ascii\", errors=\"strict\", newline=\"\") as f:\n",
        "        for line in f:\n",
        "            addr = line.strip()\n",
        "            if not addr:\n",
        "                continue\n",
        "            s.add(ipv6_to_packed(addr))\n",
        "    return len(s)\n",
        "\n",
        "# тест по примеру из условия\n",
        "input_path = \"test_input.txt\"\n",
        "count_unique_naive(input_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Промежуточный вывод\n",
        "На маленьком примере наивный подход работает. Но для 10⁹ строк он не годится из-за памяти.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Основное решение: внешняя сортировка + подсчёт уникальных при слиянии\n",
        "\n",
        "Идея внешней сортировки:\n",
        "1) читаем вход потоком и накапливаем в памяти **чанк** из K адресов;\n",
        "2) сортируем чанк и записываем в временный файл (\"run\") в бинарном виде (каждый адрес — 16 байт);\n",
        "3) получаем много отсортированных runs на диске;\n",
        "4) дальше делаем **k-way merge** (слияние отсортированных файлов) и\n",
        "   - либо сливаем в более крупные runs (если их слишком много),\n",
        "   - либо сразу считаем уникальные (сравнивая с предыдущим значением).\n",
        "\n",
        "Важное наблюдение: если данные отсортированы, то одинаковые значения идут подряд.\n",
        "Значит, уникальные считаются одной переменной `prev` и счётчиком.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1. Запись одного отсортированного run\n",
        "\n",
        "Будем хранить run как бинарный файл, где подряд идут записи по 16 байт.\n",
        "Это компактно и быстро.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flush_run(buf: List[bytes], run_idx: int) -> str:\n",
        "    \"\"\"Отсортировать буфер 16-байтных IPv6 и записать на диск как run-файл.\"\"\"\n",
        "    buf.sort()\n",
        "    path = os.path.join(f\"run_{run_idx:06d}.bin\")\n",
        "    with open(path, \"wb\", buffering=1024 * 1024) as f:\n",
        "        for rec in buf:\n",
        "            f.write(rec)  # ровно 16 байт\n",
        "    return path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2. Генерация начальных отрезков из входного файла\n",
        "\n",
        "Читаем входной текстовый файл и нарезаем его на чанки по `chunk_records` записей.\n",
        "Каждый чанк сортируется и пишется на диск.\n",
        "\n",
        "Память регулируется параметром `chunk_records`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_initial_runs(input_path: str, chunk_records: int) -> List[str]:\n",
        "    runs: List[str] = []\n",
        "    buf: List[bytes] = []\n",
        "    run_idx = 0\n",
        "\n",
        "    with open(input_path, \"r\", encoding=\"ascii\", errors=\"strict\", newline=\"\") as fin:\n",
        "        for line in fin:\n",
        "            s = line.strip()\n",
        "            if not s:\n",
        "                continue\n",
        "            buf.append(ipv6_to_packed(s))\n",
        "            if len(buf) >= chunk_records:\n",
        "                runs.append(flush_run(buf, run_idx))\n",
        "                run_idx += 1\n",
        "                buf.clear()\n",
        "\n",
        "    if buf:\n",
        "        runs.append(flush_run(buf, run_idx))\n",
        "        buf.clear()\n",
        "\n",
        "    return runs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3. k-way merge: слияние нескольких runs в один run\n",
        "\n",
        "Слияние делается через мин-кучу (heap):\n",
        "- из каждого файла читаем первый 16-байтный адрес;\n",
        "- кладём в кучу пары `(значение, индекс_файла)`;\n",
        "- вынимаем минимум, пишем в выход, дочитываем следующий из того же файла.\n",
        "\n",
        "Так мы получаем один отсортированный файл из нескольких отсортированных.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _open_runs(run_paths: List[str]) -> List[BinaryIO]:\n",
        "    files: List[BinaryIO] = []\n",
        "    for p in run_paths:\n",
        "        files.append(open(p, \"rb\", buffering=1024 * 1024))\n",
        "    return files\n",
        "\n",
        "def merge_runs_to_file(run_paths: List[str], out_path: str) -> None:\n",
        "    files = _open_runs(run_paths)\n",
        "    try:\n",
        "        heap: List[Tuple[bytes, int]] = []\n",
        "        for i, f in enumerate(files):\n",
        "            rec = f.read(RECORD_SIZE)\n",
        "            if rec:\n",
        "                heap.append((rec, i))\n",
        "        heapq.heapify(heap)\n",
        "\n",
        "        with open(out_path, \"wb\", buffering=1024 * 1024) as out:\n",
        "            while heap:\n",
        "                rec, i = heapq.heappop(heap)\n",
        "                out.write(rec)\n",
        "                nxt = files[i].read(RECORD_SIZE)\n",
        "                if nxt:\n",
        "                    heapq.heappush(heap, (nxt, i))\n",
        "    finally:\n",
        "        for f in files:\n",
        "            try:\n",
        "                f.close()\n",
        "            except Exception:\n",
        "                pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4. Если runs слишком много: многоступенчатое слияние (reduce)\n",
        "\n",
        "В реальности нельзя открыть одновременно тысячи файлов.\n",
        "Поэтому мы уменьшаем число runs, сливая их партиями по `fan_in`.\n",
        "\n",
        "После каждого слияния удаляем старые файлы\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reduce_runs(run_paths: List[str], fan_in: int) -> List[str]:\n",
        "    level = 0\n",
        "    runs = run_paths[:]\n",
        "\n",
        "    while len(runs) > fan_in:\n",
        "        new_runs: List[str] = []\n",
        "        for batch_start in range(0, len(runs), fan_in):\n",
        "            batch = runs[batch_start:batch_start + fan_in]\n",
        "            merged_path = os.path.join(f\"merge_{level:03d}_{batch_start // fan_in:06d}.bin\")\n",
        "            merge_runs_to_file(batch, merged_path)\n",
        "            new_runs.append(merged_path)\n",
        "\n",
        "            # Удаляем старые run-файлы\n",
        "            for p in batch:\n",
        "                try:\n",
        "                    os.remove(p)\n",
        "                except FileNotFoundError:\n",
        "                    pass\n",
        "\n",
        "        runs = new_runs\n",
        "        level += 1\n",
        "\n",
        "    return runs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5. Подсчёт уникальных при k-way merge\n",
        "\n",
        "Когда у нас осталось \"не слишком много\" runs, можно открыть их одновременно и\n",
        "сделать финальное k-way слияние, **не записывая результат на диск**, а сразу считая уникальные.\n",
        "\n",
        "Для этого поддерживаем `prev` — предыдущий адрес, и если текущий `rec != prev`, увеличиваем счётчик.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_unique_across_runs(run_paths: List[str]) -> int:\n",
        "    if not run_paths:\n",
        "        return 0\n",
        "\n",
        "    files = _open_runs(run_paths)\n",
        "    try:\n",
        "        heap: List[Tuple[bytes, int]] = []\n",
        "        for i, f in enumerate(files):\n",
        "            rec = f.read(RECORD_SIZE)\n",
        "            if rec:\n",
        "                heap.append((rec, i))\n",
        "        heapq.heapify(heap)\n",
        "\n",
        "        prev: Optional[bytes] = None\n",
        "        uniq = 0\n",
        "\n",
        "        while heap:\n",
        "            rec, i = heapq.heappop(heap)\n",
        "            if prev != rec:\n",
        "                uniq += 1\n",
        "                prev = rec\n",
        "            nxt = files[i].read(RECORD_SIZE)\n",
        "            if nxt:\n",
        "                heapq.heappush(heap, (nxt, i))\n",
        "\n",
        "        return uniq\n",
        "    finally:\n",
        "        for f in files:\n",
        "            try:\n",
        "                f.close()\n",
        "            except Exception:\n",
        "                pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6. Итоговая функция решения задачи\n",
        "\n",
        "Объединяем всё в одну функцию:\n",
        "1) создаём временную директорию;\n",
        "2) генерируем initial runs;\n",
        "3) уменьшаем их число многоступенчатым слиянием;\n",
        "4) считаем уникальные финальным k-way merge;\n",
        "5) пишем ответ в выходной файл.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_unique_ipv6_external(input_path: str,\n",
        "                               output_path: str,\n",
        "                               chunk_records: int = 1_000_000,\n",
        "                               fan_in: int = 128) -> int:\n",
        "\n",
        "    runs = generate_initial_runs(input_path, chunk_records)\n",
        "    runs = reduce_runs(runs, fan_in)\n",
        "    ans = count_unique_across_runs(runs)\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as fout:\n",
        "        fout.write(str(ans) + \"\\n\")\n",
        "\n",
        "    return ans\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Проверка решения на примере из условия\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: 4\n"
          ]
        }
      ],
      "source": [
        "output_path = \"test_output.txt\"\n",
        "ans = count_unique_ipv6_external(input_path, output_path, chunk_records=2, fan_in=8)\n",
        "print(\"Ответ:\", ans)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "На тесте из условия 4, как и требуется.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Эксперимент на сгенерированных данных\n",
        "\n",
        "Данные генерировались с помощью команды\n",
        "\n",
        "`python .\\data\\task-1\\generate_data.py input.txt 12596 100000000`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e350f112",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Считаем уникальные внешней сортировкой...\n",
            "Ответ: 12596\n",
            "Время подсчёта: 2.94 сек\n"
          ]
        }
      ],
      "source": [
        "in_path = os.path.join(\"input.txt\")\n",
        "out_path = os.path.join(\"output.txt\")\n",
        "\n",
        "total_lines = 100000000      \n",
        "unique_count = 12596   # число уникальных IPv6 внутри файла\n",
        "\n",
        "print(\"Считаем уникальные внешней сортировкой...\")\n",
        "t2 = time.time()\n",
        "ans = count_unique_ipv6_external(in_path, out_path, chunk_records=100_000, fan_in=64)\n",
        "t3 = time.time()\n",
        "print(f\"Ответ: {ans}\")\n",
        "print(f\"Время подсчёта: {t3 - t2:.2f} сек\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3498353b",
      "metadata": {},
      "source": [
        "### Результаты лежат в папке experiment_cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Итоговые выводы\n",
        "\n",
        "1) **Канонизация** IPv6 выполняется через `socket.inet_pton(AF_INET6, s)` и даёт однозначные 16 байт.\n",
        "2) Для огромных файлов (до 10⁹ строк) нельзя хранить все адреса в памяти → нужен дисковый алгоритм.\n",
        "3) **Внешняя сортировка** решает задачу при ограниченной RAM:\n",
        "   - нарезаем вход на чанки, сортируем и сохраняем как runs;\n",
        "   - выполняем многоступенчатое k-way слияние;\n",
        "   - считаем уникальные при финальном merge за один проход.\n",
        "4) Память управляется параметром `chunk_records`, число одновременно открытых файлов — `fan_in`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
